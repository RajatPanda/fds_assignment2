{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzK831HHSMNLO6OoV4jJ9N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajatPanda/fds_assignment2/blob/main/Assignment2_G24AI2073_RajatBhusanPanda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Java, Hadoop & mrjob"
      ],
      "metadata": {
        "id": "F4TFHDVvDQop"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5cSy5G3jwCev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "6ac9d959-b206-49b8-c95e-653ccf72f89f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Hadoop 3.3.6\n",
            "Source code repository https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c\n",
            "Compiled by ubuntu on 2023-06-18T08:22Z\n",
            "Compiled on platform linux-x86_64\n",
            "Compiled with protoc 3.7.1\n",
            "From source with checksum 5652179ad55f76cb287d9c633bb53bbd\n",
            "This command was run using /content/hadoop-3.3.6/share/hadoop/common/hadoop-common-3.3.6.jar\n"
          ]
        }
      ],
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install -y openjdk-8-jdk-headless -qq\n",
        "!wget -q https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
        "!tar -xf hadoop-3.3.6.tar.gz\n",
        "import os, pathlib, sys, json, re, textwrap, math, itertools\n",
        "os.environ[\"JAVA_HOME\"]  = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/content/hadoop-3.3.6\"\n",
        "!pip install -q mrjob==0.7.4\n",
        "!ln -sf /content/hadoop-3.3.6/bin/* /usr/bin/\n",
        "!hadoop version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get CSVs"
      ],
      "metadata": {
        "id": "c4sAuKcGDZig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O cruise.csv  https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/cruise.csv\n",
        "!wget -q -O churn.csv   https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/customer_churn.csv\n",
        "!wget -q -O ecommerce.csv https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/e-com_customer.csv"
      ],
      "metadata": {
        "id": "ourlXR4VtUqD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mrjob.job import MRJob\n",
        "from mrjob.protocol import RawValueProtocol\n",
        "from io import StringIO, BytesIO"
      ],
      "metadata": {
        "id": "wjQ2Gx_JdGaY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function for local runner"
      ],
      "metadata": {
        "id": "YNdwvKs0DxoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_local(mr_cls, sample_str):\n",
        "    runner = mr_cls(args=['-r', 'inline', '--no-conf'])\n",
        "    stdin  = BytesIO(sample_str.encode('utf-8'))\n",
        "    with runner.make_runner() as r:\n",
        "        r._stdin = stdin\n",
        "        r.run()\n",
        "        return [line.decode('utf-8').strip() for line in r.cat_output()]"
      ],
      "metadata": {
        "id": "azD3DXjPckv2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: For each Cruise line compute:\n",
        "* ship_count\n",
        "* avg_tonnage\n",
        "* max_crew\n",
        "\n",
        "Uses a combiner to lower shuffle volume."
      ],
      "metadata": {
        "id": "2MrAwYdeD9tO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file cruiseline_agg.py\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "from mrjob.protocol import RawValueProtocol\n",
        "import csv, sys\n",
        "class CruiseLineAgg(MRJob):\n",
        "    \"\"\"Q1: For each Cruise line compute\n",
        "       (a) ship_count\n",
        "       (b) avg_tonnage\n",
        "       (c) max_crew\n",
        "       Uses a combiner to lower shuffle volume.\"\"\"\n",
        "    OUTPUT_PROTOCOL = RawValueProtocol\n",
        "    def mapper(self, _, line):\n",
        "        if line.startswith('Ship_name'):\n",
        "            return\n",
        "        row = next(csv.reader([line]))\n",
        "        line_name = row[1]\n",
        "        tonnage   = float(row[3])\n",
        "        crew      = float(row[8])\n",
        "        yield line_name, (1, tonnage, crew)\n",
        "    def combiner(self, line_name, vals):\n",
        "        c, t_sum, c_max = 0, 0.0, 0.0\n",
        "        for cnt, ton, cr in vals:\n",
        "            c += cnt; t_sum += ton; c_max = max(c_max, cr)\n",
        "        yield line_name, (c, t_sum, c_max)\n",
        "    def reducer(self, line_name, vals):\n",
        "        c, t_sum, c_max = 0, 0.0, 0.0\n",
        "        for cnt, ton, cr in vals:\n",
        "            c += cnt; t_sum += ton; c_max = max(c_max, cr)\n",
        "        avg_ton = round(t_sum / c, 2)\n",
        "        yield None, f\"Cruise Line: {line_name}| Ship Count: {c}| Average Tonnage: {avg_ton:.2f}| Max Crew: {c_max}\"\n",
        "    def steps(self):\n",
        "        return [MRStep(mapper=self.mapper,\n",
        "                       combiner=self.combiner,\n",
        "                       reducer=self.reducer)]\n",
        "if __name__ == '__main__':\n",
        "    CruiseLineAgg.run()"
      ],
      "metadata": {
        "id": "OvYCzJ9iaa3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "751b4f4d-1d74-4042-d0cc-ac4f65558c4c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cruiseline_agg.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from cruiseline_agg import CruiseLineAgg\n",
        "\n",
        "sample = (\n",
        "    \"Ship_name,Cruise_line,Age,Tonnage,passengers,length,cabins,passenger_density,crew\\n\"\n",
        "    \"Journey,Azamara,6,30.276999999999997,6.94,5.94,3.55,42.64,3.55\\n\"\n",
        "    \"Quest,Azamara,6,30.276999999999997,6.94,5.94,3.55,42.64,3.55\\n\"\n",
        "    \"Celebration,Carnival,26,47.262,14.86,7.22,7.43,31.8,6.7\\n\"\n",
        ")\n",
        "\n",
        "output_lines = run_local(CruiseLineAgg, sample)\n",
        "for line in output_lines:\n",
        "    print(line)\n"
      ],
      "metadata": {
        "id": "RjIKQDUmeBrx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa00cb2f-c5f6-4204-f5a4-ec335117ea72"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:mrjob.conf:No configs specified for inline runner\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cruise Line: Azamara| Ship Count: 2| Average Tonnage: 30.28| Max Crew: 3.55\n",
            "\n",
            "Cruise Line: Carnival| Ship Count: 1| Average Tonnage: 47.26| Max Crew: 6.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: Multi-step job that filters to VIP firms (from distributed cache) then emits company-level churn rate to 4-decimal precision."
      ],
      "metadata": {
        "id": "hI8TwQQkFV-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file churn_rate.py\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import csv, os\n",
        "class CompanyChurnRate(MRJob):\n",
        "    \"\"\"Q2: Multi-step job that filters to VIP firms (from distributed\n",
        "       cache) then emits company-level churn rate to 4-decimal precision.\"\"\"\n",
        "    FILES = ['vip_companies.txt']\n",
        "    def configure_args(self):\n",
        "        super().configure_args()\n",
        "        self.add_file_arg('--vip', default='vip_companies.txt',\n",
        "                          help='VIP company list, one per line')\n",
        "    def load_options(self, args):\n",
        "        super().load_options(args)\n",
        "        with open(self.options.vip) as f:\n",
        "            self.vip = {l.strip() for l in f if l.strip()}\n",
        "    def mapper(self, _, line):\n",
        "        if line.lower().startswith('customerid'):\n",
        "            return\n",
        "        row = next(csv.reader([line]))\n",
        "        company = row[2]\n",
        "        churned = int(row[-1] == '1')\n",
        "        if company in self.vip:\n",
        "            yield (company, 'TOTAL'), 1\n",
        "            if churned: yield (company, 'CHURNED'), 1\n",
        "    def reducer(self, key, vals):\n",
        "        yield key, sum(vals)\n",
        "    def calc_rate(self, company, pairs):\n",
        "        pairs = dict(pairs)\n",
        "        rate  = pairs.get('CHURNED',0) / pairs.get('TOTAL',1)\n",
        "        yield None, f\"Company: {company}| Churn Rate: {rate:.4f}\"\n",
        "    def steps(self):\n",
        "        return [MRStep(mapper=self.mapper,\n",
        "                       reducer=self.reducer),\n",
        "                MRStep(reducer=self.calc_rate)]\n",
        "if __name__ == '__main__':\n",
        "    CompanyChurnRate.run()"
      ],
      "metadata": {
        "id": "SpX6krhvuoX1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f85bb2cd-e968-4529-d89a-8845958d344b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing churn_rate.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from churn_rate import CompanyChurnRate\n",
        "\n",
        "sample = (\n",
        "    \"Names,Age,Total_Purchase,Account_Manager,Years,Num_Sites,Onboard_date,Location,Company,Churn\\n\"\n",
        "    \"Cameron Williams,42.0,11066.8,0,7.22,8.0,2013-08-30 07:00:40,\\\"10265 Elizabeth Mission Barkerburgh, AK 89518\\\",Harvey LLC,1\\n\"\n",
        "    \"Kevin Mueller,41.0,11916.22,0,6.5,11.0,2013-08-13 00:38:46,\\\"6157 Frank Gardens Suite 019 Carloshaven, RI 17756\\\",Wilson PLC,1\\n\"\n",
        "    \"Eric Lozano,38.0,12884.75,0,6.67,12.0,2016-06-29 06:20:07,\\\"1331 Keith Court Alyssahaven, DE 90114\\\",\\\"Miller, Johnson and Wallace\\\",1\\n\"\n",
        "    \"Phillip White,42.0,8010.76,0,6.71,10.0,2014-04-22 12:43:12,\\\"13120 Daniel Mount Angelabury, WY 30645-4695\\\",Smith Inc,1\\n\"\n",
        ")\n",
        "\n",
        "output_lines = run_local(CompanyChurnRate, sample)\n",
        "for line in output_lines:\n",
        "    print(line)"
      ],
      "metadata": {
        "id": "f7Eb60IFFhuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3: Extract 2-letter state code from Address, sum Yearly Amount Spent, output top-5."
      ],
      "metadata": {
        "id": "A0x8AqE_G03v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file state_spend.py\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep, MRStep\n",
        "import csv, re, heapq\n",
        "STATE_RE = re.compile(r',\\s*([A-Z]{2})\\s+\\d{5}$')\n",
        "class StateSpend(MRJob):\n",
        "    \"\"\"Q3: Extract 2-letter state code from Address,\n",
        "       sum Yearly Amount Spent, output top-5.\"\"\"\n",
        "    def mapper(self, _, line):\n",
        "        if line.startswith('Email'):\n",
        "            return\n",
        "        row = next(csv.reader([line]))\n",
        "        addr, amt = row[1], float(row[-1])\n",
        "        m = STATE_RE.search(addr)\n",
        "        if m:\n",
        "            yield m.group(1), amt\n",
        "    def reducer_sum(self, state, amts):\n",
        "        yield None, (sum(amts), state)\n",
        "    def reducer_top5(self, _, state_amt_pairs):\n",
        "        top = heapq.nlargest(5, state_amt_pairs)\n",
        "        for amt, st in top:\n",
        "            yield None, f\"{st}\\t{amt:.2f}\"\n",
        "    def steps(self):\n",
        "        return [MRStep(mapper=self.mapper,\n",
        "                       reducer=self.reducer_sum),\n",
        "                MRStep(reducer=self.reducer_top5)]\n",
        "if __name__ == '__main__':\n",
        "    StateSpend.run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyPWc9H07ahU",
        "outputId": "5ca4b2b2-a634-4e9a-8155-71f8ca2a2eea"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing state_spend.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from state_spend import StateSpend\n",
        "\n",
        "sample = (\n",
        "  \"Email,Address,Avatar,Avg Session Length,Time on App,Time on Website,Length of Membership,Yearly Amount Spent\\n\"\n",
        "  \"mstephenson@fernandez.com,\\\"835 Frank TunnelWrightmouth, MI 82180-9605\\\",Violet,34.49726772511229,12.65565114916675,39.57766801952616,4.0826206329529615,587.9510539684005\\n\"\n",
        "  \"hduke@hotmail.com,\\\"4547 Archer CommonDiazchester, CA 06566-8576\\\",DarkGreen,31.92627202636016,11.109460728682564,37.268958868297744,2.66403418213262,392.2049334443264\\n\"\n",
        "  \"pallen@yahoo.com,\\\"24645 Valerie Unions Suite 582Cobbborough, DC 99414-7564\\\",Bisque,33.000914755642675,11.330278057777512,37.110597442120856,4.104543202376424,487.54750486747207\\n\"\n",
        "  \"riverarebecca@gmail.com,\\\"1414 David ThroughwayPort Jason, OH 22070-1220\\\",SaddleBrown,34.30555662975554,13.717513665142507,36.72128267790313,3.120178782748092,581.8523440352177\\n\"\n",
        "  \"mstephens@davidson-herman.com,\\\"14023 Rodriguez PassagePort Jacobville, PR 37242-1057\\\",MediumAquaMarine,33.33067252364639,12.795188551078114,37.53665330059473,4.446308318351434,599.4060920457634\\n\"\n",
        "  \"alvareznancy@lucas.biz,\\\"645 Martha Park Apt. 611Jeffreychester, MN 67218-7250\\\",FloralWhite,33.871037879341976,12.026925339755056,34.47687762925054,5.493507201364199,637.102447915074\\n\"\n",
        ")\n",
        "\n",
        "output_lines = run_local(StateSpend, sample)\n",
        "for line in output_lines:\n",
        "    print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9UISSNxHbtq",
        "outputId": "c48a758f-793f-4e1b-c4eb-0441a575cf96"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:mrjob.conf:No configs specified for inline runner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4:\n",
        "* filter ships with passenger_density>35.0\n",
        "* compute per-line median length (2-dec)."
      ],
      "metadata": {
        "id": "_euXmnglIi47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file median_length.py\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import csv, statistics\n",
        "class MedianLength(MRJob):\n",
        "    \"\"\"Q4: (1) filter ships with passenger_density>35.0\n",
        "       (2) compute per-line median length (2-dec).\"\"\"\n",
        "    def mapper(self, _, line):\n",
        "        if line.startswith('Ship_name'):\n",
        "            return\n",
        "        row = next(csv.reader([line]))\n",
        "        line_name = row[1]\n",
        "        density   = float(row[7])\n",
        "        if density > 35.0:\n",
        "            yield line_name, float(row[5])   # length\n",
        "    def reducer(self, line_name, lengths):\n",
        "        ls = sorted(lengths)\n",
        "        med = statistics.median(ls)\n",
        "        yield None, f\"{line_name}\\t{med:.2f}\"\n",
        "    def steps(self):\n",
        "        return [MRStep(mapper=self.mapper, reducer=self.reducer)]\n",
        "if __name__ == '__main__':\n",
        "    MedianLength.run()\n"
      ],
      "metadata": {
        "id": "NEPIBrAA7eke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run every job on Hadoop inside Colab"
      ],
      "metadata": {
        "id": "bOHCOlUfIdIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python cruiseline_agg.py   cruise.csv      -r hadoop  --output-dir q1_out  --no-conf --quiet\n",
        "python churn_rate.py        churn.csv       -r hadoop  --vip vip_companies.txt --output-dir q2_out --no-conf --quiet\n",
        "python state_spend.py       ecommerce.csv   -r hadoop  --output-dir q3_out  --no-conf --quiet\n",
        "python median_length.py     cruise.csv      -r hadoop  --output-dir q4_out  --no-conf --quiet\n",
        "\n",
        "echo \"=== Q1 ===\"; hdfs dfs -cat q1_out/part* | head\n",
        "echo \"=== Q2 ===\"; hdfs dfs -cat q2_out/part* | head\n",
        "echo \"=== Q3 ===\"; hdfs dfs -cat q3_out/part* | head\n",
        "echo \"=== Q4 ===\"; hdfs dfs -cat q4_out/part* | head\n"
      ],
      "metadata": {
        "id": "JcOZwZqf7su_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}